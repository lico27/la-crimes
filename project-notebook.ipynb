{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Predictive Analytics: Solving a Real World Problem with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: Los Angeles Crime Data 2010-19 and 2020-25\n",
    "2010-19 dataset: https://catalog.data.gov/dataset/crime-data-from-2010-to-2019      \n",
    "2020-25 dataset (accessed up to 25/05/2025): https://catalog.data.gov/dataset/crime-data-from-2020-to-present\n",
    "\n",
    "## Research Question: How can we protect children from being victims of crime in Los Angeles?\n",
    "\n",
    "The model will predict the risk level of a child becoming a victim of crime, based on demographic factors (such as age, sex, and descent) in combination with spatial and temporal variables (such as location, time of day, and day of the week).\n",
    "\n",
    "Real-world interventions can be based on the predictions of the model. For example, if the model predicts that there is a high risk level for Black children being victimised in Central LA during weekday evenings, a local youth centre could implement targeted outreach programmes during those hours — offering safe spaces, support services, or structured activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor \n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 2010-19 data from csv\n",
    "df1 = pd.read_csv(\"la_crimes_2010-19.csv\")\n",
    "\n",
    "#get 2020-25 data from csv\n",
    "df2 = pd.read_csv(\"la_crimes_2020-25.csv\")\n",
    "\n",
    "#clean variable names\n",
    "df1 = (\n",
    "    df1.clean_names()\n",
    "    .rename(columns={\"date_occ\":\"date\", \"time_occ\":\"time\", \"area_name\":\"area\", \"crm_cd\":\"crime_code\", \"crm_cd_desc\":\"crime_type\", \"premis_cd\":\"premises_code\", \"premis_desc\":\"premises_type\", \"weapon_used_cd\":\"weapon_code\", \"weapon_desc\":\"weapon_type\"})\n",
    "    )\n",
    "\n",
    "df2 = (\n",
    "    df2.clean_names()\n",
    "    .rename(columns={\"date_occ\":\"date\", \"time_occ\":\"time\", \"area\":\"area_\", \"area_name\":\"area\", \"crm_cd\":\"crime_code\", \"crm_cd_desc\":\"crime_type\", \"premis_cd\":\"premises_code\", \"premis_desc\":\"premises_type\", \"weapon_used_cd\":\"weapon_code\", \"weapon_desc\":\"weapon_type\"})\n",
    "    )\n",
    "\n",
    "#join dataframes and view all columns\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "First I will remove true duplicates, as each instance should have a unique identifier (according to the metadata, this is `dr_no` - Division of Records number). Any row that is a complete duplicate is therefore likely to be attributable to a data entry error. \n",
    "\n",
    "Next, I will view the unique values that appear in the demographic variables to check whether they need cleaning, using the metadata to support my understanding and decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get demographic values and counts\n",
    "vict_sex_values = df['vict_sex'].value_counts(dropna=False).to_dict()\n",
    "vict_descent_values = df['vict_descent'].value_counts(dropna=False).to_dict()\n",
    "\n",
    "print(f\"Victim Sex Values:\\n{vict_sex_values}\\n\\nVictim Descent Values:\\n{vict_descent_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy victim sex variable\n",
    "vict_sex_map = {\n",
    "    \"M\": \"Male\",\n",
    "    \"F\": \"Female\",\n",
    "    \"X\": \"Other/Unknown\",\n",
    "    \"H\": \"Other/Unknown\",\n",
    "    \"N\": \"Other/Unknown\",\n",
    "    \"-\": \"Other/Unknown\"\n",
    "}\n",
    "df[\"vict_sex\"] = df[\"vict_sex\"].map(vict_sex_map).fillna(\"Other/Unknown\")\n",
    "\n",
    "#tidy victim descent variable\n",
    "vict_descent_map = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latin/Mexican\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\",\n",
    "    \"-\": \"Unknown\"\n",
    "}\n",
    "df[\"vict_descent\"] = df[\"vict_descent\"].map(vict_descent_map).fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates\n",
    "df[\"date_rptd\"] = pd.to_datetime(df[\"date_rptd\"], format=\"%m/%d/%Y %I:%M:%S %p\").dt.normalize()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%m/%d/%Y %I:%M:%S %p\").dt.normalize()\n",
    "\n",
    "#convert times\n",
    "df = df[df[\"time\"] > 99]\n",
    "df[\"time\"] = df[\"time\"].astype(str).str.zfill(4)\n",
    "\n",
    "#get datetime column\n",
    "df[\"datetime_str\"] = df[\"date\"].dt.strftime(\"%Y-%m-%d\") + \" \" + df[\"time\"].str[:2] + \":\" + df[\"time\"].str[2:]\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime_str\"], format=\"%Y-%m-%d %H:%M\")\n",
    "df.drop(columns=\"datetime_str\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will drop rows with missing victim age, as this variable is essential for building my model. I will also drop rows where the age is zero or less (vict_age contains many 0s and negative numbers, possibly as crimes without known/human victims e.g. vandalism) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns that won't be used for the model\n",
    "df = df.drop(columns=[\"dr_no\", \"date_rptd\", \"area_\", \"rpt_dist_no\", \"part_1_2\", \"crime_code\", \"mocodes\", \"premises_code\", \"weapon_code\", \"status\", \"status_desc\", \"crm_cd_1\", \"crm_cd_2\", \"crm_cd_3\", \"crm_cd_4\", \"location\", \"cross_street\"])\n",
    "\n",
    "#drop rows with missing victim age\n",
    "df = df.dropna(subset=[\"vict_age\"])\n",
    "\n",
    "#drop rows where age is zero or less\n",
    "df = df[df[\"vict_age\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Categorical Data\n",
    "\n",
    "#### Demographics\n",
    "\n",
    "The `vict_sex` requires no grouping as there are three categories, none of which are too low frequency.\n",
    "\n",
    "However, there are several low-frequency values in `vict_descent`. Knowing that this will be a feature in my model, and to avoid anonymisation issues, I will combine some of the lower-frequency ethnicities\n",
    "\n",
    "\n",
    "Groups that are under 23k (1% of the dataset)\n",
    "\n",
    "\n",
    "\n",
    " values with fewer than 30 observations into \"Other\". This approach will help protect the validity of future tests and models; multiple low-frequency groups in this context would represent too small a sample size for each value to draw meaningful conclusions (Krishnan, 2011).\n",
    "\n",
    " I grouped different Asian ethnicities to reduce data sparsity and avoid skewing the model. While I recognise the distinct experiences of subgroups, aggregating was necessary to ensure statistical reliability when predicting child victimisation, as supported in UK government guidance when justified.\n",
    "\n",
    " In this project, I have chosen to group individuals of various Asian descents into a single broader category. This decision was made with care and awareness of the sensitivities involved, as different Asian subgroups—such as Indian, Chinese, Pakistani, Bangladeshi, and others—have distinct cultural, socioeconomic, and historical backgrounds. However, the decision was ultimately based on methodological needs rather than sociopolitical assumptions. Aggregating categories was necessary to ensure sufficient sample size for model training and to avoid introducing noise or skew into the predictive model due to sparsity. The primary objective of the analysis is to explore child victimisation patterns using machine learning, and for this purpose, overly granular ethnic categories could lead to unreliable or misleading outputs. While disaggregation may be more appropriate in some social or policy contexts, grouping was deemed the best option here to maintain model robustness and analytical clarity, in line with statistical practices outlined by the UK government when appropriate justifications exist for doing so [source: UK Government Ethnicity Data Guidance, 2020]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vict_descent_map_2 = {\n",
    "    \"Other Asian\": \"Asian\",\n",
    "    \"Chinese\": \"Asian\",\n",
    "    \"Cambodian\": \"Asian\",\n",
    "    \"Filipino\": \"Asian\",\n",
    "    \"Guamanian\": \"Other/Unknown\",\n",
    "    \"American Indian/Alaskan Native\": \"Other/Unknown\",\n",
    "    \"Japanese\": \"Asian\",\n",
    "    \"Korean\": \"Asian\",\n",
    "    \"Laotian\": \"Asian\",\n",
    "    \"Pacific Islander\": \"Other/Unknown\",\n",
    "    \"Samoan\": \"Other/Unknown\",\n",
    "    \"Hawaiian\": \"Other/Unknown\",\n",
    "    \"Vietnamese\": \"Asian\",\n",
    "    \"Asian Indian\": \"Asian\",\n",
    "    \"Other\": \"Other/Unknown\",\n",
    "    \"Unknown\": \"Other/Unknown\"\n",
    "}\n",
    "df[\"vict_descent\"] = df[\"vict_descent\"].map(vict_descent_map_2).combine_first(df[\"vict_descent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weapon Type\n",
    "\n",
    "There are 80 unique weapon types in the dataset. I felt that this was a small enough number to handle mostly manually, so I defined categories that I felt to be logical. For efficienct, I asked ChatGPT to categorise each of the weapon types into my pre-defined categories (OpenAI, 2025). I edited the dictionary slightly to tweak the decisions made by ChatGPT, to ensure that the categorisation was logical, e.g. changing \"SYRINGE\" from \"Burning/Toxic Substance\" to \"Knife/Blade/Sharp Object\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define weapon categories\n",
    "weapon_map = {\n",
    "    \"STRONG-ARM (HANDS, FIST, FEET OR BODILY FORCE)\": \"Bodily Force\",\n",
    "    \"UNKNOWN WEAPON/OTHER WEAPON\": \"Other/Unknown/No Weapon Used\",\n",
    "    \"VERBAL THREAT\": \"Verbal Threat\",\n",
    "    \"HAND GUN\": \"Gun/Firearm\",\n",
    "    \"KNIFE WITH BLADE 6INCHES OR LESS\": \"Knife/Blade/Sharp Object\",\n",
    "    \"SEMI-AUTOMATIC PISTOL\": \"Gun/Firearm\",\n",
    "    \"OTHER KNIFE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"UNKNOWN FIREARM\": \"Gun/Firearm\",\n",
    "    \"VEHICLE\": \"Vehicle\",\n",
    "    \"MACE/PEPPER SPRAY\": \"Burning/Toxic Substance\",\n",
    "    \"BOTTLE\": \"Blunt/Hitting Object\",\n",
    "    \"STICK\": \"Blunt/Hitting Object\",\n",
    "    \"ROCK/THROWN OBJECT\": \"Blunt/Hitting Object\",\n",
    "    \"CLUB/BAT\": \"Blunt/Hitting Object\",\n",
    "    \"FOLDING KNIFE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"REVOLVER\": \"Gun/Firearm\",\n",
    "    \"KITCHEN KNIFE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"BLUNT INSTRUMENT\": \"Blunt/Hitting Object\",\n",
    "    \"KNIFE WITH BLADE OVER 6 INCHES IN LENGTH\": \"Knife/Blade/Sharp Object\",\n",
    "    \"PIPE/METAL PIPE\": \"Blunt/Hitting Object\",\n",
    "    \"AIR PISTOL/REVOLVER/RIFLE/BB GUN\": \"Gun/Firearm\",\n",
    "    \"SIMULATED GUN\": \"Gun/Firearm\",\n",
    "    \"BELT FLAILING INSTRUMENT/CHAIN\": \"Blunt/Hitting Object\",\n",
    "    \"OTHER CUTTING INSTRUMENT\": \"Knife/Blade/Sharp Object\",\n",
    "    \"HAMMER\": \"Blunt/Hitting Object\",\n",
    "    \"PHYSICAL PRESENCE\": \"Bodily Force\",\n",
    "    \"SCREWDRIVER\": \"Knife/Blade/Sharp Object\",\n",
    "    \"MACHETE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"UNKNOWN TYPE CUTTING INSTRUMENT\": \"Knife/Blade/Sharp Object\",\n",
    "    \"SCISSORS\": \"Knife/Blade/Sharp Object\",\n",
    "    \"OTHER FIREARM\": \"Gun/Firearm\",\n",
    "    \"CONCRETE BLOCK/BRICK\": \"Blunt/Hitting Object\",\n",
    "    \"SHOTGUN\": \"Gun/Firearm\",\n",
    "    \"RIFLE\": \"Gun/Firearm\",\n",
    "    \"FIXED OBJECT\": \"Blunt/Hitting Object\",\n",
    "    \"STUN GUN\": \"Gun/Firearm\",\n",
    "    \"BOARD\": \"Blunt/Hitting Object\",\n",
    "    \"FIRE\": \"Burning/Toxic Substance\",\n",
    "    \"GLASS\": \"Blunt/Hitting Object\",\n",
    "    \"SWITCH BLADE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"CAUSTIC CHEMICAL/POISON\": \"Burning/Toxic Substance\",\n",
    "    \"BRASS KNUCKLES\": \"Blunt/Hitting Object\",\n",
    "    \"AXE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"TIRE IRON\": \"Blunt/Hitting Object\",\n",
    "    \"SCALDING LIQUID\": \"Burning/Toxic Substance\",\n",
    "    \"TOY GUN\": \"Gun/Firearm\",\n",
    "    \"RAZOR BLADE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"SWORD\": \"Knife/Blade/Sharp Object\",\n",
    "    \"BOMB THREAT\": \"Verbal Threat\",\n",
    "    \"RAZOR\": \"Knife/Blade/Sharp Object\",\n",
    "    \"ICE PICK\": \"Knife/Blade/Sharp Object\",\n",
    "    \"HECKLER & KOCH 93 SEMIAUTOMATIC ASSAULT RIFLE\": \"Gun/Firearm\",\n",
    "    \"ASSAULT WEAPON/UZI/AK47/ETC\": \"Gun/Firearm\",\n",
    "    \"DIRK/DAGGER\": \"Knife/Blade/Sharp Object\",\n",
    "    \"LIQUOR/DRUGS\": \"Other/Unknown/No Weapon Used\",\n",
    "    \"EXPLOXIVE DEVICE\": \"Burning/Toxic Substance\",\n",
    "    \"AUTOMATIC WEAPON/SUB-MACHINE GUN\": \"Gun/Firearm\",\n",
    "    \"SAWED OFF RIFLE/SHOTGUN\": \"Gun/Firearm\",\n",
    "    \"STARTER PISTOL/REVOLVER\": \"Gun/Firearm\",\n",
    "    \"ROPE/LIGATURE\": \"Other/Unknown/No Weapon Used\",\n",
    "    \"SEMI-AUTOMATIC RIFLE\": \"Gun/Firearm\",\n",
    "    \"CLEAVER\": \"Knife/Blade/Sharp Object\",\n",
    "    \"BOWIE KNIFE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"DOG/ANIMAL (SIC ANIMAL ON)\": \"Other/Unknown/No Weapon Used\",\n",
    "    \"DEMAND NOTE\": \"Verbal Threat\",\n",
    "    \"STRAIGHT RAZOR\": \"Knife/Blade/Sharp Object\",\n",
    "    \"BLACKJACK\": \"Blunt/Hitting Object\",\n",
    "    \"SYRINGE\": \"Knife/Blade/Sharp Object\",\n",
    "    \"BOW AND ARROW\": \"Other/Unknown/No Weapon Used\",\n",
    "    \"MARTIAL ARTS WEAPONS\": \"Blunt/Hitting Object\",\n",
    "    \"UNK TYPE SEMIAUTOMATIC ASSAULT RIFLE\": \"Gun/Firearm\",\n",
    "    \"UZI SEMIAUTOMATIC ASSAULT RIFLE\": \"Gun/Firearm\",\n",
    "    \"RELIC FIREARM\": \"Gun/Firearm\",\n",
    "    \"HECKLER & KOCH 91 SEMIAUTOMATIC ASSAULT RIFLE\": \"Gun/Firearm\",\n",
    "    \"ANTIQUE FIREARM\": \"Gun/Firearm\",\n",
    "    \"MAC-10 SEMIAUTOMATIC ASSAULT WEAPON\": \"Gun/Firearm\",\n",
    "    \"MAC-11 SEMIAUTOMATIC ASSAULT WEAPON\": \"Gun/Firearm\",\n",
    "    \"M1-1 SEMIAUTOMATIC ASSAULT RIFLE\": \"Gun/Firearm\",\n",
    "    \"M-14 SEMIAUTOMATIC ASSAULT RIFLE\": \"Gun/Firearm\"\n",
    "}\n",
    "\n",
    "#map to dataframe, drop original column\n",
    "df[\"weapon_group\"] = df[\"weapon_type\"].map(weapon_map).fillna(\"Other/Unknown/No Weapon Used\")\n",
    "df.drop(\"weapon_type\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime Type\n",
    "\n",
    "There are 142 crime types in the dataset, which I felt was too many to deal with completely manually, but I noticed that many of them had repeating words (e.g. \"THEFT\") so I wrote a function to group them by keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to group crimes\n",
    "def crime_grouping(crime):\n",
    "    if pd.isna(crime):\n",
    "        return \"Other\"\n",
    "    crime = crime.upper()\n",
    "    if any(word in crime for word in [\"CHILD\"]):\n",
    "        return \"Offense Against a Child\"\n",
    "    elif any(word in crime for word in [\"HOMICIDE\", \"MANSLAUGHTER\", \"LYNCHING\"]):\n",
    "        return \"Murder/Manslaughter\"\n",
    "    elif any(word in crime for word in [\"ASSAULT\", \"BRANDISH\", \"SHOTS\", \"BATTERY\", \"BOMB\"]):\n",
    "        return \"Assault/Violence\"\n",
    "    elif any(word in crime for word in [\"THEFT\", \"BURGLARY\", \"ROBBERY\", \"STOLEN\", \"EXTORTION\", \"PICKPOCKET\", \"SNATCHING\", \"BUNCO\", \"FRAUD\", \"COUNTERFEIT\"]):\n",
    "        return \"Theft-Related\"\n",
    "    elif any(word in crime for word in [\"VANDALISM\", \"ARSON\"]):\n",
    "        return \"Property Damage\"\n",
    "    elif any(word in crime for word in [\"VIOLATION\", \"TRESPASSING\", \"DISTURBING\", \"CONTEMPT\", \"THROWING\", \"RESISTING\", \"STALKING\", \"PROWLER\", \"THREAT\"]):\n",
    "        return \"Public Order/Threatening Behaviour\"\n",
    "    elif any(word in crime for word in [\"LEWD\", \"SEX\", \"RAPE\", \"PENETRATION\", \"INDECENT\", \"COPULATION\", \"PEEPING\", \"PIMPING\"]):\n",
    "        return \"Sexual Offence\"\n",
    "    elif any(word in crime for word in [\"KIDNAPPING\", \"IMPRISONMENT\", \"TRAFFICKING\"]):\n",
    "        return \"Kidnapping/Trafficking\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "#apply function to dataframe, drop original column\n",
    "df[\"crime_group\"] = df[\"crime_type\"].apply(crime_grouping).fillna(\"Other\")\n",
    "df.drop(\"crime_type\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premises Type\n",
    "\n",
    "There are 319 premises types, with very little possibility for grouping using the same methods as above, as the vast majority have unique names with few repeating words. As such, I decided to use semantic similarity clustering. \n",
    "\n",
    "\n",
    "\n",
    "a hybrid approach combining semantic similarity clustering with manual corrections. First, I employed a sentence transformer model (all-MiniLM-L6-v2) to encode both the unique premises types and 12 predefined categories that better reflected the nature of the data. Each premises type was then assigned to the most semantically similar category based on cosine similarity between their embeddings. However, given the complexity and variety of the premises descriptions, this automated approach produced several misclassifications. To address this, I implemented a correction function that uses keyword matching to reassign premises types that contained specific identifying words to their appropriate categories. For example, any premises containing 'BANK' was reassigned to 'Financial', while those containing words like 'HOME', 'DRIVEWAY', or 'GARAGE' were moved to 'Residence/Private Outdoor Space'. This two-stage process combined the efficiency of automated semantic clustering with the accuracy of domain-specific manual corrections, ensuring that the final groupings were both comprehensive and contextually appropriate.\n",
    "\n",
    "There are 319 premises types, with very little possibility for grouping using the same methods as above, as the vast majority have unique names with few repeating words. As such, I decided to use semantic similarity clustering as a data preprocessing step to create meaningful categorical features for the machine learning model. Using a sentence transformer model (all-MiniLM-L6-v2), I automatically grouped premises types into 12 predefined categories based on semantic similarity. However, due to the complexity and variety of premises descriptions, I implemented additional keyword-based corrections to ensure accurate categorization. This preprocessing approach reduced the dimensionality from 319 unique premises types to 12 meaningful categories, creating a more manageable and interpretable feature for predicting when and where children are most likely to be victims of crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define preferred clusters\n",
    "categories = [\"Residence/Private Outdoor Space\", \"Street/Public Outdoor Space\", \"Transport Hub/Vehicle\", \"Restaurant/Eatery\", \"Store/Mall/Business\", \"Education\", \"Public Services/Healthcare\", \"Place of Worship\", \"Leisure/Entertainment/Sport\", \"Online\", \"Financial\", \"Other\"]\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "unique_premises = df[\"premises_type\"].dropna().unique()\n",
    "\n",
    "premises_embeddings = model.encode(unique_premises)\n",
    "category_embeddings = model.encode(categories)\n",
    "\n",
    "type_clusters = {}\n",
    "for i, premise in enumerate(unique_premises):\n",
    "    similarities = np.dot(premises_embeddings[i], category_embeddings.T)\n",
    "    best_category = categories[np.argmax(similarities)]\n",
    "    type_clusters[premise] = best_category\n",
    "\n",
    "df[\"premises_group\"] = df[\"premises_type\"].map(type_clusters)\n",
    "\n",
    "#define function to regroup incorrect clusters\n",
    "def premises_grouping(premises, current_group):\n",
    "    if pd.isna(premises):\n",
    "        return \"Other\"\n",
    "    premises = premises.upper()\n",
    "    if any(word in premises for word in [\"BANK\"]):\n",
    "        return \"Financial\"\n",
    "    elif any(word in premises for word in [\"PUBLIC STORAGE\", \"DIY\", \"VALET\", \"OFFICE\", \"RADIO\", \"FACTORY\", \"MARKET\", \"OTHER BUSINESS\", \"CONNECTION\", \"SALES\", \"BMW\", \"CAR WASH\", \"GROVE\", \"EQUIPMENT\", \"COURIER\"]):\n",
    "        return \"Store/Mall/Business\"\n",
    "    elif any(word in premises for word in [\"HOME\", \"DRIVEWAY\", \"PATIO\", \"PORCH\", \"FOSTER\", \"GARAGE\", \"MOBILE\", \"BALCONY\", \"PROJECT\"]):\n",
    "        return \"Residence/Private Outdoor Space\"\n",
    "    elif any(word in premises for word in [\"FIRE\", \"SEWAGE\", \"CLINIC\", \"LIBRARY\", \"HOSPITAL\", \"MORTUARY\", \"HOSPICE\", \"ENERGY\", \"CARE\", \"WATER\", \"JAIL\", \"POLICE\", \"DENTAL\", \"RECYCLING\"]):\n",
    "        return \"Public Services/Healthcare\"\n",
    "    elif any(word in premises for word in [\"HARBOR\", \"LINE\", \"PARKING\", \"TRAM\", \"AIRCRAFT\", \"CHARTER\", \"MTA\"]):\n",
    "        return \"Transport Hub/Vehicle\"\n",
    "    elif any(word in premises for word in [\"RINK\", \"BASKETBALL\", \"ARCADE\", \"COCKTAIL\", \"MUSEUM\", \"STAPLES\", \"STADIUM\", \"BEVERLY\", \"VACATION\", \"HOTEL\", \"MOTEL\", \"BOWLING\"]):\n",
    "        return \"Leisure/Entertainment/Sport\"\n",
    "    elif any(word in premises for word in [\"ALLEY\", \"TRASH\", \"TUNNEL\", \"PAYPHONE\", \"FREEWAY\", \"GATHERING\", \"TRANSIENT\", \"BEACH\", \"RESERVOIR\", \"RIVER\", \"BRIDGE\", \"OTHER/OUTSIDE\"]):\n",
    "        return \"Street/Public Outdoor Space\"\n",
    "    elif any(word in premises for word in [\"COFFEE\"]):\n",
    "        return \"Restaurant/Eatery\"\n",
    "    elif any(word in premises for word in [\"SWAP\", \"ESCALATOR\", \"STAIR\", \"ELEVATOR\", \"ABATEMENT\", \"TACTICAL\", \"RETIRED\", \"SHED\"]):\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return current_group\n",
    "\n",
    "#apply function to dataframe, drop original column\n",
    "df[\"premises_group\"] = df.apply(lambda row: premises_grouping(row[\"premises_type\"], row[\"premises_group\"]), axis=1).fillna(\"Other\")\n",
    "df.drop(\"premises_type\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
